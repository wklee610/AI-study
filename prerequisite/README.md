# [AI 용어 정리]

**인공지능 (Artifical Intelligence)**
- 학습하고 추론할 수 있는 지능을 가진 컴퓨터 시스템을 만드는 기술

**강인공지능 vs 약인공지능**
- 강인공지능: 인공일반지능, 사람의 지능과 유사 (혼자 모든 것을 다 하는 AI)
- 약인공지능: 특정 분야에서 사람을 돕는 보조 AI (음성 비서, 자율주행 etc.)

**머신러닝 (Machine Learning)**
- 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야 (scikit-learn)

**딥러닝 (Deep Learning)**
- 인공신경망(Artificial Neural Network, ANN)을 기반으로 한 머신러닝 분야 
- 인공신경망과 동의어로 사용되는 경우가 많으며 혹은 심층신경망 (Deep Neural Network, DNN)이라고도 함
- 라이브러리: [Tensorflow](https://www.tensorflow.org/?hl=ko)
- 라이브러리: [Pytorch](https://pytorch.org/)

- - -
## [머신러닝 (Machine Learning)]

**이진 분류 (Binary Classification)**
- 머신러닝에서 여러 개의 종류 (혹은 클래스) 중 하나를 구별해 내는 문제를 Classification
- 2개의 클래스 중 하나를 고르는 문제

**특성 (Feature)**
- 데이터를 표현하는 특징

**[Matplotlib](https://matplotlib.org/)**
- 파이썬 과학계산용 그래프 패키지

**k-최근접 이웃 알고리즘 (k-Nearest Neighbors Algorithm, KNN)**
- 머신러닝 알고리즘
- 인접한 샘플을 기반으로 예측을 수행

**훈련 (Training)**
- 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정 또는 모델에 데이터를 전달하여 규칙을 학습하는 과정

**지도학습 (Supervised Learning)**
- 입력(데이터)과 정답으로 이루어진 훈련 데이터가 필요하며, 새로운 데이터를 예측하는데 활용

**비지도학습 (Unsupervised Learning)**
- 타깃 데이터 없이 입력 데이터만 있을 때 사용
- 정답을 사용하지 않으므로, 맞힐 수 없지만 데이터를 잘 파악하거나 변형하는데 도움

**훈련 데이터 (Traning Data)**
- 필요한 입력(데이터)와 타깃을 합쳐 놓은 것

**훈련 세트 / 테스트 세트 (Train Set / Test Set)**
- 모델을 훈련할 때는 훈련 세트를 사용하고 평가는 테스트 세트로 진행
- 테스트 세트는 전체 데이터에서 20 ~ 30%

**샘플링 편향 (Sampling Bias)**
- 훈련 세트와 테스트 세트에 샘플링이 고르게 섞여 있지 않을 때, 샘플링 편향 나타냄
- 지도 학습 모델을 만들 수 없음

**[NumPy](https://numpy.org/)**
- 파이썬 배열 라이브러리
- 고차원의 배열을 손쉽게 만들고 조작

**데이터 전처리 (Data Preprocessing)**
- 머신러닝 모델에 훈련 데이터를 주입하기 전 가공하는 단계
- 특성값을 일정한 기준으로 맞추어 주는 작업
- 데이터를 표현하는 기준이 다르면 알고리즘을 올바르게 예측 불가능

**브로드캐스팅 (Broadcasting)**
- 조건을 만족하면 모양이 다른 배열 간의 연산을 가능하게 해주는 기능

**회귀 (regression)**
- 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 문제

**k-최근접 이웃 회귀 (k-Nearest Neighbors Regression)**
- k-최근접 이웃 알고리즘 (k-Nearest Neighbors Algorithm)을 사용하여 회귀 문제를 품

**결정계수 (Coefficient of Determination, R^2)**
- 회귀 모델에서 예측의 적합도를 0과 1 사이의 값으로 계산한 것으로 1에 가까울수록 완벽
- R^2 = 1 - (Target - Predict)^2 / (Target - Average)^2

**과대적합 (Overfitting)**
- 과대적합은 모델의 훈련 세트 점수가 테스트 세트 점수보다 훨씬 높을 경우

**과소적합 (Underfitting)**
- 모델의 훈련 세트와 테스트 세트 점수가 모두 동일하게 낮거나 테스트 세트 성능이 오히려 더 높을 경우를 의미함

**선형 회귀 (Linear Regression)**
- 대표적인 회귀 알고리즘
- 특성이 하나인 경우 어떤 직선을 학습하는 알고리즘

**가중치 (weight) or 계수 (Coefficient)**
- 선형 회귀가 학습한 직선의 기울기를 종종 가중치 또는 계수라고 함

**다항 회귀 (Polynomial Regression)**
- 다항식을 사용하여 특성 (Feature)과 타깃 (Target)사이의 관계를 나타낸 선형 회귀

**다중 회귀 (Multiple Regression)**
- 여러 개의 특성을 사용한 선형 회귀

**변환기 (Transformer)**
- 특성을 만들거나 전처리하는 scikit-learn의 클래스로 타깃 데이터 없이 입력 데이터를 변환함

**릿지 회귀 (Ridge Regression)**
- 규제가 있는 선형 회귀 모델
- 모델 객체를 만들 때 alpha 매개변수로 규제의 강도를 조절
- alpha 값이 크면 규제 강도가 세지므로 계수 값을 더 줄이고조금 더 과소적합되도록 유도하여 과대적합을 완화시킴

**하이퍼파라미터 (Hyperparameter)**
- 머신러닝 모델이 학습할 수 없고 사람이 지정하는 파라미터

**라쏘 회귀 (Lasso Regression)**
- 규제가 있는 선형 회귀 모델
- alpha 매개변수로 규제의 강도를 조절
- 릿지와 달리 계수 값을 아예 0으로 만들 수 있음

**다중 분류 (Multi-class Classification)**
- 타깃 데이터에 2개 이상의 클래스가 포함된 문제

**로지스틱 회귀 (Logistic Regression)**
- 선형 방정식을 사용한 분류 알고리즘
- 선형 회귀와 달리 시그모이드 함수 or 소프트맥스 함수를 사용하여 클래스 화률 출력

**시그모이드 함수 (Sigmoid Function)**
- 시그모이드 함수 or 로지스틱 함수 라고 불림
- 선형 방정식의 출력을 0과 1 사이의 값으로 압축하며 이진 분류를 위해 사용
- 이진 분류일 경우 시그모이드 함수의 출력이 0.5보다 크면 양성 클래스
- 0.5보다 작으면 음성 클래스로 판단
- ø = 1 / (1 + e^(-2))

**불리언 인덱싱 (Boolean Indexing)**
- Numpy 배열에서 True, False 값을 전달하여 행을 선택

**소프트맥스 함수 (Softmax Function)**
- 여러 개의 선형 방정식의 출력값을 0~1 사이로 압축하고 전체 합이 1이 되도록 만듬
- 이를 위해 지수 함수를 사용하기 때문에, 정규화된 지수 함수라고 함

**확률적 경사 하강법 (Stochastic Gradient Descent)**
- 훈련 세트에서 랜덤하게 하나의 샘플을 선택하여 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘

**에포크 (Epoch)**
- 확률적 경사 하강법에서 훈련 세트를 한 번 모두 사용하는 과정

**미니배치 경사 하강법 (Minibatch Gradient Descent)**
- 1개가 아닌 여러 개의 샘플을 사용해 경사 하강법을 수행하는 방법으로 실전에서 많이 사용

**배치 경사 하강법 (Batch Gradient Descent)**
- 한 번에 전체 샘플을 사용하는 방법
- 전체 ㅈ데이터를 사용하므로 가장 안정적인 방법
- 하지만 그만큼 컴퓨터 자원을 많이 사용
- 데이터가 매우 방대하면 전체 데이터를 처리하기 힘듦

**손실 함수 (Loss Function)**
- 어떤 문제에서 머신러닝 알고리즘이 얼마나 효과가 없는지 측정하는 기준

**로지스틱 손실 함수 (Logistic Loss Function)**
- 양성 클래스 (Target = 1)일 때, 손실은 -log (예측 확률)로 계싼하며, 1 확률이 1에서 멀어질수록 손실은 아주 큰 양수가 됨
- 음성 클래스 (Target = 0)일 때, 손실은 -log (1 - 예측 확률)로 계산하며 예측 확률이 0에서 멀어질수록 손실은 아주 큰 양수가 됨

**크로스엔트로피 손실 함수 (Cross-Entropy Loss Function)**
- 다중 분류에서 사용하는 손실 함수

**힌지 손실 (Hinge Loss)**
- 서포트 벡터 머신 (Support Vector Machine)이라 불리는 머신러닝 알고리즘을 위한 손실 함수
- SGDClassifier가 여러 종류의 손실 함수를 loss 매개변수에 지정하여 다양한 머신러닝 알고리즘을 지원함

**결정 트리 (Decision Tree)**
- 하나씩 정답을 맞춰가며 학습하는 알고리즘
- 예측 과정을 이해하기 쉬움

**검증 세트 (Validation Set)**
- 하이퍼파라미터 튜닝을 위해 모델을 평가할 때, 테스트 세트를 사용하지 않기 위해 훈련 세트에서 다시 떼어 낸 데이터 세트
- Train set 60%, Test set 20%, Validation set 20%

**교차 검증 (Cross Validation)**
- 훈련 세트를 여러 폴드로 나눈 다음 한 홀드가 검증 세트의 역할을 하고 나머지 폴드에서는 모델을 훈련함
- 모델 1에서는 a, b 훈련 세트와 c 검증 세트로 모델 평가
- 모델 2에서는 a, c 훈련 세트와 b 검증 세트로 모델 평가
- 모델 3에서는 b, c 훈련 세트와 a 검증 세트로 모델 평가
- 검증 점수 평균

**그리드 서치 (Grid Search)**
- 하이퍼파라미터 탐색을 자동화 해주는 도구

**랜덤 서치 (Random Search)**
- 연속적인 매개변수 값을 탐색할 때 유용

**정형 데이터 (Structed Data)**
- 특정 구조로 이루어진 데이터

**비정형 데이터 (Unstructed Data)**
- 정형화되기 어려운 사진이나 음악 등의 데이터

**앙상블 학습 (Ensamble Learning)**
- 여러 알고리즘을 합쳐서 성능을 높이는 머신러닝 기법

**랜덤 포레스트 (Random Forest)**
- 대표적인 결정 트리 기반의 앙상블 학습 방법
- 안정적인 성능
- 랜덤하게 일부 특성을 선택하여 트리를 만드는 것이 특징

**부트스트랩 샘플 (Bootstrap Sample)**
- 데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식

**엑스트라 트리 (Extra Trees)**
- 랜덤 포레스트와 비슷하게 동작하며 결정 트리를 사용하여 앙상블 모델을 만듦
- 부트스트랩 샘플을 사용하지 않는 대신 랜덤하게 노드를 분할하여 과대적합을 감소시킴

**그레디언트 부스팅 (Gradient Boosting)**
- 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법
- 깊이가 얕은 결정 트리를 사용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있음

**히스토그램 기반 그레디언트 부스팅 (Histogram-based Gradient Boosting)**
- 그레디언트 부스팅의 속도를 개선
- 과대적합을 잘 억제하며 그레디언트 부스팅보다 높은 성능 제공
- 안정적인 결과와 높은 성능으로 높은 인기

- - -
## [딥러닝 (Deep Learning)]
**히스토그램 (Histogram)**
- 값이 발생한 빈도를 그래프로 표시한 것
- x축 = 값의 구간
- y축 = 발생 빈도

**군집 (Clustering)**
- 비슷한 샘플끼리 그룹으로 모으는 작업
- 대표적인 비지도 학습 작업 중 하나

**k-평균 알고리즘 (k-means Algorithm)**
- 처음에 랜덤하게 클러스터 중심을 정하여 클러스터를 만들고
- 클러스터의 중심을 이동하여 다시 클러스터를 결정하는 식으로 반복해서 최적의 클러스터를 구성하는 알고리즘

**이너셔 (inertia)**
- k-평균 알고리즘은 클러스터 중심과 클러스터에 속한 샘플 사이의 거리를 잴 수 있는데 이 거리의 제곱 합을 이너셔라고 함
- 즉, 클러스터의 샘플이 얼마나 가깝게 있는지를 나타내는 값

**주성분 분석 (Principla Component Analysis, PCA)**
- 차원 축소 알고리즘의 하나로 데이터에서 가장 분산이 큰 방향을 찾는 방법이며 이런 방향을 주성분이라고 함
- 원본 데이터를 주성분에 투영하여 새로운 특성을 만들 수 있음

**인공신경망 (Artificial Neural Network, ANN)**
- 생물학적 뉴런에서 영감을 받아 만든 머신러닝 알고리즘
- 신경망은 기존의 머신러닝 알고리즘으로 다루기 어려웠던 이미지, 음성, 텍스트 분야에서 뛰어난 성능을 발휘

**[Tensorflow](https://www.tensorflow.org/?hl=ko)**
- 구글이 만든 딥러닝 라이브러리
- CPU와 GPU를 사용하여 인공신경망 모델을 효율적으로 훈련하며 모델 구축과 서비스에 필요한 다양한 도구를 제공함
- 케라스 (Keras)를 핵심 API로 채택

**[Pytorch](https://pytorch.org/)**
- 페이스북(Facebook) / 메타(Meta) 가 만든 딥러닝 라이브러리
- CPU와 GPU를 사용하여 인공신경망 모델을 효율적으로 훈련하며 모델 구축과 서비스에 필요한 다양한 도구를 제공함

**활성화 함수 (Activation Function)**
- 소프트맥스와 같이 뉴런의 선형 방정식 계산 결과에 적용되는 함수

**원-핫 인토딩 (One-Hot Encoding)**
- 타깃값을 해당 클래스만 1이고 나머지는 모두 0인 배열로 만드는 것
- 다중 분류에서 크로스 엔트로피 손실 함수를 사용하려면 0, 1, 2와 같이 정수로 된 타깃값을 원-핫 인코딩으로 변환

**은닉층 (Hidden Layer)**
- 입력층과 출력층 사이에 있는 모든 층을 은닉층이라고 부름

**심층 신경망 (Deep Neural Network, DNN)**
- 2개 이상의 층을 포함한 신경망
- 종종 다층 인공신경망, 심층 신경망, 딥러닝을 같은 의미로 사용

**렐루 함수 (ReLU Function)**
- 입력이 양수일 경우 마치 활성화 함수가 없는 것처럼 그냥 입력을 통과시키고 음수일 경우에는 0으로 만드는 함수

**옵티마이저 (Optimizer)**
- 신경망의 가중치와 절편을 학습시키기 위한 알고리즘

**적응적 학습률 (Adaptive Learning Rate)**
- 모델이 최적점에 가까이 갈수록 안정적으로 수렴하도록 학습률을 낮추도록 조정하는 방법
- 학습률 매개변수를 튜닝하는 수고를 덜 수 있는 장점

**드롭아웃 (Dropout)**
- 훈련 과정에서 층에 있는 일부 뉴런을 랜덤하게 꺼서 (뉴런의 출력을 0으로 만듦) 과대적합을 막음

**콜백 (Callback)**
- 케라스에서 훈련 과정 중간에 어떤 작업을 수행할 수 있게 하는 객체
- keras.callbacks 패키지 아래에 있는 클래스로 fit() 메서드의 callbacks 매개변수에 리스트로 전달하여 사용

**합성곱 (Convolution)**
- 밀집층과 비슷하게 입력과 가중치를 곱하고 절편을 더하는 선형 계산이지만 밀집층과 달리 합성곱은 입력 데이터 전체에 가중치를 적용하는 것이 아니라 일부에 가중치를 곱함

**필터 (Filter)**
- 밀집층의 뉴런에 해당
- 뉴런 = 필터 = 커널

**특성 맵 (Feature Map)**
- 합성곱 계산을 통해 얻은 출력

**패딩 / 세임 패딩 (Padding / Same Padding)**
- 입력 배열의 주위를 가상의 원소 (0)로 채우는 것
- 합성곱 신경망에서는 세임 패딩을 많이 사용

**밸리드 패딩 (Valid Padding)**
- 패딩 없이 순수한 입력 배열에서만 합성곱을 하여 특성 맵을 만드는 경우
- 특성 맵의 크기가 줄어들 수 밖에 없음

**스트라이드 (Stride)**
- 합성곱 층에서 필터가 입력 위를 이동하는 크기
- 기본으로 스트라이드는 1 픽셀, 한 칸씩 이동

**풀링 (Pooling)**
- 합성곱 층에서 만든 특성 맵의 가로세로 크기를 줄이는 역할을 수행
- 특성 맵의 갯수는 줄이지 않음
- 가중치가 없는 대신 특성 맵에서 최댓값이나 평균값을 선택

**최대 풀링 (Max Pooling)**
- 풀링을 수행할 때 가장 큰 값을 고를때

**평균 풀링 (Average Pooling)**
- 폴링을 수행할 때 평균값을 계산할 때

**순차 데이터 (Sequential Data)**
- 텍스트나 시계열 데이터와 같이 순서에 의미가 있는 데이터

**시계열 데이터 (Time Series Data)**
- 일정한 시간 간격으로 기록된 데이터

**피드포워드 신경망 (Feedforward Neural Network, FFNN)**
- 입력 데이터의 흐름이 앞으로만 전달되는 신경망
- 완전 연결 신경망과 합성곱 신경망이 모두 피드포워드 신경망에 속함

**순환 신경망 (Recurrent Neural Network, RNN)**
- 완전 연결 신경망과 거의 비슷
- 순차 데이터에 잘 맞는 인공신경망의 한 종류
- 순차 데이터를 처리하기 위해 고안된 순환 층을 1개 이상 사용한 신경망

**셀 (Cell)**
- 순환 신경망에서는 특별히 층을 셀이라 부름
- 한 셀에는 여러 개의 뉴런이 있지만 완전 연결 신경망과 달리 뉴런을 모두 표시하지 않고 하나의 셀로 층을 표현

**은닉 상태 (Hidden State)**
- 순환 신경망에서 셀의 출력을 은닉 상태라 부름
- 은닉 상태는 다음 층으로 전달 될 뿐만 아니라 셀이 다음 타임스텝의 데이터를 처리할 때 재사용

**말뭉치 (Corpus)**
- 자연어 처리 분야에서 훈련 데이터를 종종 말뭉치라고 부름

**토큰 (Token)**
- 일반적으로 영어 문장은 모두 소문자로 바꾸고 구둣점을 삭제한 다음 공백을 기준으로 분리
- 이렇게 텍스트에서 공백으로 구분되는 문자열 또는 단어를 토큰이라고 부름

**단어 임베딩 (Word Embedding)**
- 순환 신경망에서 텍스트를 처리할 때 즐겨 사용하는 방법으로 입력으로 정수 데이터를 받아 메모리를 훨씬 효율적으로 사용

**LSTM (Long Short-Term Memory)**
- 단기 기억을 오래 기억하기 위해 고안된 순환층
- 입력 게이트, 삭제 게이트, 출력 게이트 역할을 하는 작은 셀 포함

**셀 상태 (Cell State)**
- LSTM 셀은 은닉 상태 외에 셀 상태를 출력
- 셀 상태는 다음 층으로 전달되지 않으며 현재 셀에만 순환